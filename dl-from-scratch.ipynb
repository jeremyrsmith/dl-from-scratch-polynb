{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "be.botkop:scorch_2.12:0.1.1"
        ]
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        
      },
      "env" : {
        "OMP_NUM_THREADS" : "1"
      }
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# Neural networks from scratch in Scala!\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 27,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "The purpose of this notebook is to explain how you can implement a simple neural network in Scala.\n",
        "\n",
        "\n",
        "<br>\n",
        "Here is some jargon, just in case.\n",
        "In its simplest form, a neural net is a sequence of matrix multiplications glued together by **activation** functions. The task of the activation function is to introduce a non-linearity (such as setting the negative values to 0) between these multiplications. The reason this works can be traced back to the Universal Approximation Theorem. The matrices are called **weights**, or **parameters**. The matrix multiplication together with the activation function is called a **layer**. Layers can be stacked together to form **modules**. The complete set of modules that compose the neural net is often called a **model**.<br>\n",
        "\n",
        "\n",
        "The neural net we'll be discussing is part of the supervised domain of machine learning. In supervised learning there is a learning, or **training** phase, and an **inference** phase.\n",
        "\n",
        "\n",
        "In the training phase, a **dataloader** pushes **labeled** input in the form of a matrix (input) and a vector (expected outcome) to the model, where the multiplications and activations ensue.  The dataloader is responsible for transforming the semantic units of both input (a picture of a cat) and output (the label 'cat') into resp. a matrix and a vector of numbers. Data is usually loaded in multiples (eg. 64 images, together with their labels). These multiples are called **minibatches**.\n",
        "\n",
        "\n",
        "A **loss function** compares the prediction of the network on each sample with the ground truth, ie. the label. It notifies the network of the errors it made, in the form of **gradients,** by a process called **backpropagation**. Subsequently an **optimizer** uses the gradients to nudge the weights into the direction of the correct outcome, by subtracting a fraction of the gradients from the weights. This fraction is called the **learning rate**.\n",
        "\n",
        "\n",
        "This is repeated on every minibatch of the training data set. A complete processing on the training set in this way is called an **epoch**. The model is trained as many epochs as needed, ie. until the loss is sufficiently low, and the accuracy (or another metric) is satisfactory. The **validation set** is a special part of the training set, that was set aside beforehand. It is not used as input for the training, but only to assess how well or how bad we are doing on data the model has never seen before. The loss and accuracy metrics are also computed on the validation set, and these are an important source for adjusting the **hyperparameters** of the model, such as the learning rate.\n",
        "\n",
        "\n",
        "In the inference phase, we use the trained model to make predictions on unseen, unlabeled data. In this phase there is no loss, no backprop, and no optimizer.![null](https://lh3.googleusercontent.com/hIvqk0GIe-dOWXiDN-oFetbGvqOROSGtkM1-RadouReF4IrZpGcTvJqGOLt8539JzKDkfB4-2RTDHsNdJo5_wDzdHY68kkpPyB07NuOsYYs4VYE6oT5A0UKgZuEoKtGwLab1rExNxt4)<br>\n",
        "\n",
        "\n",
        "<br>\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 28,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Using a numerical library\n",
        "\n",
        "\n",
        "In order to build anything similar to the above design, we will need a numerical library. Numsca is a library for the Scala programming language, inspired by NumPy, with\n",
        "\n",
        "\n",
        "* support for matrices and tensors (multi-dimensional arrays)\n",
        "* a collection of high-level mathematical functions to operate on these tensors\n",
        "* broadcasting and fancy indexing, just like NumPy.\n",
        "\n",
        "\n",
        "Numsca can be found at [https://github.com/botkop/numsca](https://github.com/botkop/numsca)\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585069828290,
          "endTs" : 1585069828355
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import botkop.{numsca => ns}\r\n",
        "import ns.Tensor\r\n",
        "import scala.language.implicitConversions"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 2,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Implementing Backpropagation\n",
        "\n",
        "\n",
        "![null](https://lh4.googleusercontent.com/ISD7nDd6dwMkQaOpwxvq6gsNP5NCFWjwzTWl4HiPW_roXGPQV0S4Ryi8KcRx5KzA1GpG7VFFOwjSf6s2Gah_r-o8nUbJXRht91qzgujt8fNMcbKcpOW-eyAPD_OA9gxP0G9jFSuGcN0)<br>\n",
        "\n",
        "The circuit diagram above shows a computation graph which allows to relate the value and gradient of the variables to the functions that produced them. In order to achieve this we will make 2 constructs that are defined in terms of each other:<br>\n",
        "\n",
        "* a Variable, which is a wrapper around a tensor. It also stores the originating Function of this Variable, if any. For example, the function will be None if `x = Variable(ns.zeros(3, 3,))`, but will be `Mul` if `x = a * b `where a and b are also Variables.\n",
        "\n",
        "* <li>&nbsp;a Function, which has a&nbsp;<code>forward()</code>&nbsp;method that produces the standard result of a computation, and a&nbsp;<code>backward(g)</code>&nbsp;method where g is a tensor that represents the upstream gradient. The gradient is distributed among the composing Variables according to calculus rules, and passed on to the&nbsp;<code>backward(g)</code>&nbsp;methods of these Variables.</li>\n",
        "\n",
        "\n",
        "### Variables & Functions\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "jupyter.outputs_hidden" : true,
        "cell.metadata.exec_info" : {
          "startTs" : 1585069830546,
          "endTs" : 1585069830766
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "/**\r\n",
        "  * Wrapper around a tensor.\r\n",
        "  * Keeps track of the computation graph by storing the originating function of this variable, if any.\r\n",
        "  * The function will be None if the variable is the result of simple wrapping around a tensor.\r\n",
        "  * If the variable is the result of a computation, then f will be set to the function behind this computation.\r\n",
        "  * For example, f will be None if x = Variable(ns.zeros(3, 3,)), but f will be Mul if x = a * b,\r\n",
        "  * where a and b are also Variables.\r\n",
        "  *\r\n",
        "  * @param data the tensor\r\n",
        "  * @param f function that produced this variable, if any.\r\n",
        "  */\r\n",
        "case class Variable(data: Tensor, f: Option[Function] = None) {\r\n",
        "\r\n",
        "  /* the local gradient */\r\n",
        "  lazy val g: Tensor = ns.zerosLike(data)\r\n",
        "\r\n",
        "  /**\r\n",
        "    * Accumulates the incoming gradient in the local gradient.\r\n",
        "    * Pushes the incoming gradient back through the network,\r\n",
        "    * by means of the originating function, if any.\r\n",
        "    * The gradient defaults to 1, as per the Calculus definition.\r\n",
        "    * @param gradOutput the gradient that is being pushed back through the network\r\n",
        "    */\r\n",
        "  def backward(gradOutput: Tensor = ns.ones(data.shape)): Unit = {\r\n",
        "    // gradients may have been broadcasted\r\n",
        "    // squash the dimensions to fit the original shape\r\n",
        "    val ug = unbroadcast(gradOutput)\r\n",
        "\r\n",
        "    // Gradients add up at forks.\r\n",
        "    // If the forward expression involves the variables x,y multiple times,\r\n",
        "    // then when we perform backpropagation we must be careful to use += instead of =\r\n",
        "    // to accumulate the gradient on these variables (otherwise we would overwrite it).\r\n",
        "    // This follows the multivariable chain rule in Calculus,\r\n",
        "    // which states that if a variable branches out to different parts of the circuit,\r\n",
        "    // then the gradients that flow back to it will add.\r\n",
        "    // http://cs231n.github.io/optimization-2/#staged\r\n",
        "    g += ug\r\n",
        "\r\n",
        "    // backprop thru the function that generated this variable, if any\r\n",
        "    for (gf <- f) gf.backward(ug)\r\n",
        "  }\r\n",
        "\r\n",
        "  def +(other: Variable): Variable = Add(this, other).forward()\r\n",
        "  def *(other: Variable): Variable = Mul(this, other).forward()\r\n",
        "  def dot(other: Variable): Variable = Dot(this, other).forward()\r\n",
        "  def t(): Variable = Transpose(this).forward()\r\n",
        "\r\n",
        "  // chain operator, allows to attach variables to functions without using call syntax\r\n",
        "  def ~>(f: Variable => Variable): Variable = f(this)\r\n",
        "  def shape: List[Int] = data.shape.toList\r\n",
        "\r\n",
        "  def unbroadcast(t: Tensor): Tensor =\r\n",
        "    if (t.shape.sameElements(data.shape)) t\r\n",
        "    else data.shape.zip(t.shape).zipWithIndex.foldLeft(t) {\r\n",
        "        case (d: Tensor, ((oi, ni), i)) if oi == ni => \r\n",
        "            d\r\n",
        "        case (d: Tensor, ((oi, ni), i)) if oi == 1 => \r\n",
        "            ns.sum(d, axis = i)\r\n",
        "        case _ =>\r\n",
        "            throw new Exception(\r\n",
        "              s\"unable to reduce broadcasted shape ${t.shape.toList} as ${data.shape.toList}\")\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "object Variable {\r\n",
        "  // turn a sequence of numbers directly into a Variable\r\n",
        "  def apply(ds: Double*): Variable = Variable(Tensor(ds:_*))\r\n",
        "}\r\n",
        "\r\n",
        "/**\r\n",
        " * The Function is the 2nd required component in keeping track of a computation graph.\r\n",
        " * It defines 2 methods: \r\n",
        " *   forward(): produces the traditional result of a computation\r\n",
        " *   backward(g): takes a gradient and distributes it to the composing \r\n",
        " *     Variables of the Function by calling their backward(g) method.\r\n",
        " */\r\n",
        "trait Function {\r\n",
        "  def forward(): Variable\r\n",
        "  def backward(g: Tensor): Unit\r\n",
        "}\r\n",
        "\r\n",
        "case class Add(v1: Variable, v2: Variable) extends Function {\r\n",
        "  def forward(): Variable = Variable(v1.data + v2.data, f = Some(this))\r\n",
        "  def backward(g: Tensor): Unit = {\r\n",
        "    v1.backward(g)\r\n",
        "    v2.backward(g)\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "case class Mul(v1: Variable, v2: Variable) extends Function {\r\n",
        "  override def forward(): Variable = Variable(v1.data * v2.data, f = Some(this))\r\n",
        "  override def backward(g: Tensor): Unit = {\r\n",
        "    val dv2 = v2.data * g\r\n",
        "    val dv1 = v1.data * g\r\n",
        "    v1.backward(dv2)\r\n",
        "    v2.backward(dv1)\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "case class Dot(v1: Variable, v2: Variable) extends Function {\r\n",
        "  val w: Tensor = v1.data\r\n",
        "  val x: Tensor = v2.data\r\n",
        "  override def forward(): Variable = Variable(w dot x, f = Some(this))\r\n",
        "  override def backward(g: Tensor): Unit = {\r\n",
        "    val dw = g dot x.T\r\n",
        "    val dx = w.T dot g\r\n",
        "    v1.backward(dw)\r\n",
        "    v2.backward(dx)\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "case class Transpose(v: Variable) extends Function {\r\n",
        "  override def forward(): Variable = Variable(v.data.transpose, Some(this))\r\n",
        "  override def backward(gradOutput: Tensor): Unit =\r\n",
        "    v.backward(gradOutput.transpose)\r\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 29,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Activation\n",
        "\n",
        "Activation functions are element-wise functions that introduce a non-linearity in the network.\n",
        "\n",
        "RELU is the most commonly used activation. It simply sets all negative values in a tensor equal to zero.\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 30,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585070805810,
          "endTs" : 1585070805897
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "case class Threshold(x: Variable, d: Double) extends Function {\n",
        "  override def forward(): Variable = Variable(ns.maximum(x.data, d), Some(this))\n",
        "  override def backward(gradOutput: Tensor): Unit = {\n",
        "    x.backward(gradOutput * (x.data > d))\n",
        "  }\n",
        "}\n",
        "\n",
        "def relu(x: Variable): Variable = Threshold(x, 0.0).forward()"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 4,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Module\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585069832362,
          "endTs" : 1585069832476
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "abstract class Module(localParameters: Seq[Variable] = Nil) {\r\n",
        "  // by default, obtain submodules through introspection\r\n",
        "  lazy val subModules: Seq[Module] =\r\n",
        "    this.getClass.getDeclaredFields.flatMap { f =>\r\n",
        "      f setAccessible true\r\n",
        "      f.get(this) match {\r\n",
        "        case module: Module => Some(module)\r\n",
        "        case _ => None\r\n",
        "      }\r\n",
        "    }\r\n",
        "\r\n",
        "  def parameters: Seq[Variable] = localParameters ++ subModules.flatMap(_.parameters)\r\n",
        "  def gradients: Seq[Tensor] = parameters.map(_.g)\r\n",
        "  def zeroGrad(): Unit = parameters.foreach(p => p.g := 0)\r\n",
        "\r\n",
        "  def forward(x: Variable): Variable\r\n",
        "  def apply(x: Variable): Variable = forward(x)\r\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 6,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "#### Linear Module\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585069834026,
          "endTs" : 1585069834112
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "case class Linear(weights: Variable, bias: Variable)\r\n",
        "  extends Module(Seq(weights, bias)) {\r\n",
        "  override def forward(x: Variable): Variable = {\r\n",
        "    (x dot weights.t()) + bias\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "object Linear {\r\n",
        "  def apply(inFeatures: Int, outFeatures: Int): Linear = {\r\n",
        "    val w = ns.randn(outFeatures, inFeatures) * math.sqrt(2.0 / outFeatures)\r\n",
        "    val weights = Variable(w)\r\n",
        "    val b = ns.zeros(1, outFeatures)\r\n",
        "    val bias = Variable(b)\r\n",
        "    Linear(weights, bias)\r\n",
        "  }\r\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 8,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "#### Extend Variable to act like a Module\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585069836694,
          "endTs" : 1585069836758
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "implicit def moduleApply[T <: Module](m: T): (Variable) => Variable = m.forward"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 10,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Optimizer\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 11,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585069841562,
          "endTs" : 1585069841633
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "abstract class Optimizer(parameters: Seq[Variable]) {\r\n",
        "  def step(epoch: Int)\r\n",
        "  def zeroGrad(): Unit = parameters.foreach(p => p.g := 0)\r\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 12,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "#### Stochastic Gradient Descent \n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 13,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585069843816,
          "endTs" : 1585069843909
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "case class SGD(parameters: Seq[Variable], learningRate: Double) extends Optimizer(parameters) {\r\n",
        "  override def step(epoch: Int): Unit =\r\n",
        "    parameters.foreach { p =>\r\n",
        "      p.data -= learningRate * p.g\r\n",
        "    } \r\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 14,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Softmax Loss function\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 15,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585069846518,
          "endTs" : 1585069846616
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "case class SoftmaxLoss(actual: Variable, target: Variable) extends Function {\r\n",
        "  val x: Tensor = actual.data\r\n",
        "  val y: Tensor = target.data.T\r\n",
        "\r\n",
        "  val shiftedLogits: Tensor = x - ns.max(x, axis = 1)\r\n",
        "  val z: Tensor = ns.sum(ns.exp(shiftedLogits), axis = 1)\r\n",
        "  val logProbs: Tensor = shiftedLogits - ns.log(z)\r\n",
        "  val n: Int = x.shape.head\r\n",
        "  val loss: Double = -ns.sum(logProbs(ns.arange(n), y)) / n\r\n",
        "\r\n",
        "  override def forward(): Variable = Variable(Tensor(loss), Some(this))\r\n",
        "\r\n",
        "  override def backward(gradOutput: Tensor /* not used */ ): Unit = {\r\n",
        "    val dx = ns.exp(logProbs)\r\n",
        "    dx(ns.arange(n), y) -= 1\r\n",
        "    dx /= n\r\n",
        "\r\n",
        "    actual.backward(dx)\r\n",
        "  }\r\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 16,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Data Loader\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 17,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585069848566,
          "endTs" : 1585069848646
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "trait DataLoader extends Iterable[(Variable, Variable)] {\r\n",
        "  def numSamples: Int\r\n",
        "  def numBatches: Int\r\n",
        "  def mode: String\r\n",
        "  def iterator: Iterator[(Variable, Variable)]\r\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 18,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "#### Fashion MNIST data loader\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 19,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585069851030,
          "endTs" : 1585069851237
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import scala.io.Source\r\n",
        "import scala.language.postfixOps\r\n",
        "import scala.util.Random\r\n",
        "\r\n",
        "class FashionMnistDataLoader(val mode: String,\r\n",
        "                             miniBatchSize: Int,\r\n",
        "                             take: Option[Int] = None,\r\n",
        "                             seed: Long = 231)\r\n",
        "    extends DataLoader {\r\n",
        "\r\n",
        "  Random.setSeed(seed)\r\n",
        "\r\n",
        "  val file: String = mode match {\r\n",
        "    case \"train\" => \"notebooks/data/fashionmnist/fashion-mnist_train.csv\"\r\n",
        "    case \"valid\"  => \"notebooks/data/fashionmnist/fashion-mnist_test.csv\"\r\n",
        "  }\r\n",
        "\r\n",
        "  val lines: List[String] = {\r\n",
        "    val src = Source.fromFile(file)\r\n",
        "    val lines = src.getLines().toList\r\n",
        "    src.close()\r\n",
        "    Random.shuffle(lines.tail) // skip header\r\n",
        "  }\r\n",
        "\r\n",
        "  val numFeatures = 784\r\n",
        "  val numEntries: Int = lines.length\r\n",
        "\r\n",
        "  val numSamples: Int = take match {\r\n",
        "    case Some(n) => math.min(n, numEntries)\r\n",
        "    case None    => numEntries\r\n",
        "  }\r\n",
        "\r\n",
        "  val numBatches: Int =\r\n",
        "    (numSamples / miniBatchSize) +\r\n",
        "      (if (numSamples % miniBatchSize == 0) 0 else 1)\r\n",
        "\r\n",
        "  val data: Seq[(Variable, Variable)] = lines\r\n",
        "    .take(take.getOrElse(numSamples))\r\n",
        "    .sliding(miniBatchSize, miniBatchSize)\r\n",
        "    .map { lines =>\r\n",
        "      val batchSize = lines.length\r\n",
        "\r\n",
        "      val xs = Array.fill[Float](numFeatures * batchSize)(elem = 0)\r\n",
        "      val ys = Array.fill[Float](batchSize)(elem = 0)\r\n",
        "\r\n",
        "      lines.zipWithIndex.foreach {\r\n",
        "        case (line, lnr) =>\r\n",
        "          val tokens = line.split(\",\")\r\n",
        "          ys(lnr) = tokens.head.toFloat\r\n",
        "          tokens.tail.zipWithIndex.foreach {\r\n",
        "            case (sx, i) =>\r\n",
        "              xs(lnr * numFeatures + i) = sx.toFloat / 255.0f\r\n",
        "          }\r\n",
        "      }\r\n",
        "\r\n",
        "      val x = Variable(Tensor(xs).reshape(batchSize, numFeatures))\r\n",
        "      val y = Variable(Tensor(ys).reshape(batchSize, 1))\r\n",
        "      (x, y)\r\n",
        "    }\r\n",
        "    .toSeq\r\n",
        "\r\n",
        "  lazy val meanImage: Tensor = {\r\n",
        "    val m = ns.zeros(1, numFeatures)\r\n",
        "    data.foreach {\r\n",
        "      case (x, _) =>\r\n",
        "        m += ns.sum(x.data, axis = 0)\r\n",
        "    }\r\n",
        "    m /= numSamples\r\n",
        "    m\r\n",
        "  }\r\n",
        "\r\n",
        "  if (mode == \"train\") zeroCenter(meanImage)\r\n",
        "\r\n",
        "  def zeroCenter(meanImage: Tensor): Unit = {\r\n",
        "    val bcm = broadcastTo(meanImage, miniBatchSize, numFeatures)\r\n",
        "\r\n",
        "    data.foreach {\r\n",
        "      case (x, _) =>\r\n",
        "        if (x.data.shape.head == miniBatchSize)\r\n",
        "          x.data -= bcm\r\n",
        "        else\r\n",
        "          x.data -= meanImage\r\n",
        "    }\r\n",
        "  }\r\n",
        "\r\n",
        "  def iterator: Iterator[(Variable, Variable)] =\r\n",
        "    Random.shuffle(data.toIterator)\r\n",
        "\r\n",
        "  def broadcastTo(t: Tensor, shape: Int*): Tensor =\r\n",
        "    new Tensor(t.array.broadcast(shape: _*))\r\n",
        "\r\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 20,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Learner\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 21,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1585069911441,
          "endTs" : 1585069911581
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import java.util.Locale\r\n",
        "\r\n",
        "case class Learner(trainDataLoader: DataLoader,\r\n",
        "                   validDataLoader: DataLoader,\r\n",
        "                   net: Module,\r\n",
        "                   optimizer: Optimizer,\r\n",
        "                   loss: (Variable, Variable) => Variable) {\r\n",
        "\r\n",
        "  Locale.setDefault(Locale.US)\r\n",
        "\r\n",
        "  def fit(numEpochs: Int): Unit = {\r\n",
        "    (0 until numEpochs) foreach { epoch =>\r\n",
        "      val t0 = System.nanoTime()\r\n",
        "      trainDataLoader.foreach { // for each mini batch\r\n",
        "        case (x, y) =>\r\n",
        "          optimizer.zeroGrad()\r\n",
        "          val yh = net(x)\r\n",
        "          val l = loss(yh, y)\r\n",
        "          l.backward()\r\n",
        "          optimizer.step(epoch) // update parameters using their gradient\r\n",
        "      }\r\n",
        "      val t1 = System.nanoTime()\r\n",
        "      val dur = (t1 - t0) / 1000000\r\n",
        "      val (ltrn, atrn) = evaluate(trainDataLoader, net)\r\n",
        "      val (ltst, atst) = evaluate(validDataLoader, net)\r\n",
        "\r\n",
        "      println(\r\n",
        "        f\"epoch: $epoch%2d duration: $dur%4dms loss: $ltst%1.4f / $ltrn%1.4f\\taccuracy: $atst%1.4f / $atrn%1.4f\")\r\n",
        "    }\r\n",
        "  }\r\n",
        "\r\n",
        "  def evaluate(dl: DataLoader,\r\n",
        "               net: Module): (Double, Double) = {\r\n",
        "    val (l, a) =\r\n",
        "      dl.foldLeft(0.0, 0.0) {\r\n",
        "        case ((lossAcc, accuracyAcc), (x, y)) =>\r\n",
        "          val output = net(x)\r\n",
        "          val guessed = ns.argmax(output.data, axis = 1)\r\n",
        "          val accuracy = ns.sum(guessed == y.data)\r\n",
        "          val cost = loss(output, y).data.squeeze()\r\n",
        "          (lossAcc + cost, accuracyAcc + accuracy)\r\n",
        "      }\r\n",
        "    (l / dl.numBatches, a / dl.numSamples)\r\n",
        "  }\r\n",
        "}\r\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 22,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Train a network\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 23,
      "metadata" : {
        "jupyter.outputs_hidden" : true,
        "cell.metadata.exec_info" : {
          "startTs" : 1584980965495,
          "endTs" : 1584981051302
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val batchSize = 256\r\n",
        "val trainDl = new FashionMnistDataLoader(\"train\", batchSize)\r\n",
        "val validDl = new FashionMnistDataLoader(\"valid\", batchSize)\r\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 24,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1584982453578,
          "endTs" : 1584982453747
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val nn: Module = new Module() {\r\n",
        "    val fc1 = Linear(784, 100)\r\n",
        "    val fc2 = Linear(100, 10)\r\n",
        "    override def forward(x: Variable): Variable = \r\n",
        "        x ~> fc1 ~> relu ~> fc2\r\n",
        "}\r\n",
        "\r\n",
        "def loss(yHat: Variable, y: Variable): Variable = SoftmaxLoss(yHat, y).forward()\r\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 25,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1584982454469,
          "endTs" : 1584982529232
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val numEpochs = 10\r\n",
        "val sgd = SGD(nn.parameters, .1)\r\n",
        "\r\n",
        "Learner(trainDl, validDl, nn, sgd, loss).fit(numEpochs)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "epoch:  0 duration: 3955ms loss: 1.2004 / 0.5082\taccuracy: 0.6042 / 0.8229\n",
            "epoch:  1 duration: 4134ms loss: 1.0216 / 0.4373\taccuracy: 0.6471 / 0.8463\n",
            "epoch:  2 duration: 4060ms loss: 0.9192 / 0.4031\taccuracy: 0.6796 / 0.8587\n",
            "epoch:  3 duration: 4340ms loss: 0.8451 / 0.3781\taccuracy: 0.7032 / 0.8667\n",
            "epoch:  4 duration: 4017ms loss: 0.8047 / 0.3585\taccuracy: 0.7190 / 0.8728\n",
            "epoch:  5 duration: 4016ms loss: 0.7962 / 0.3478\taccuracy: 0.7176 / 0.8769\n",
            "epoch:  6 duration: 3941ms loss: 0.8497 / 0.3383\taccuracy: 0.6967 / 0.8804\n",
            "epoch:  7 duration: 3961ms loss: 0.7662 / 0.3256\taccuracy: 0.7289 / 0.8848\n",
            "epoch:  8 duration: 3968ms loss: 0.7495 / 0.3150\taccuracy: 0.7377 / 0.8886\n",
            "epoch:  9 duration: 3937ms loss: 0.7482 / 0.3100\taccuracy: 0.7358 / 0.8901\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 26,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1584979939961,
          "endTs" : 1584979940011
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    }
  ]
}